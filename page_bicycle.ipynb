{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "hEZ7sSaR1K0h",
        "rROMLDyE1PMr",
        "kiyF27xbqwwm"
      ],
      "authorship_tag": "ABX9TyNrCGwvT5VIrraGgf9ctsRv",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rich-hyun/Battle-of-the-Strongest-Statisticians/blob/main/page_bicycle.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#사이트 기재"
      ],
      "metadata": {
        "id": "hEZ7sSaR1K0h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import xgboost as xgb\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "import numpy as np\n",
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.linear_model import LinearRegression\n",
        "import lightgbm as lgb\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore', category=UserWarning, message='.*lightgbm.*')\n",
        "\n",
        "#데이터 값 받기\n",
        "Get_year = int(input())\n",
        "Get_month = int(input())\n",
        "Get_date = int(input())\n",
        "Get_selectedCity=0\n",
        "Get_selectedModel=2\n",
        "\n",
        "data = \"2023-10-27 서울 랜덤포레스트 서울(108) 2023-10-27 15.7 NULL 1.9 71.1 10.07 NULL 16.5\"\n",
        "\n",
        "# 사용자 입력 데이터 처리 함수\n",
        "def process_user_data(data):\n",
        "    data_parts = data.split()\n",
        "    year, mnth, day = map(int, data_parts[0].split('-'))\n",
        "    values = [float(val) if val != \"NULL\" else np.nan for val in data_parts[5:12]]\n",
        "    avg_tmp, day_p, avg_wind, avg_rhum, t_sd, snow, avg_gtmp = values\n",
        "\n",
        "    if mnth in [12, 1, 2]:\n",
        "        season = 3\n",
        "    elif mnth in [3, 4, 5]:\n",
        "        season = 0\n",
        "    elif mnth in [6, 7, 8]:\n",
        "        season = 1\n",
        "    elif mnth in [9, 10, 11]:\n",
        "        season = 2\n",
        "    result=pd.DataFrame([{'year': year, 'mnth': mnth, 'day': day, 'avg_tmp': avg_tmp, 'day_p': day_p,\n",
        "                          'avg_wind': avg_wind, 'avg_rhum': avg_rhum, 't_sd': t_sd, 'snow': snow,\n",
        "                          'avg_gtmp': avg_gtmp, 'season': season}])\n",
        "    result = result.fillna(result.mean())\n",
        "    return result\n",
        "\n",
        "def process_and_encode_data(data, df):\n",
        "    # 사용자 데이터 처리\n",
        "    result_df = process_user_data(data)\n",
        "\n",
        "    # 연도, 월, 일 컬럼 추출\n",
        "    result_df['year'] = Get_year\n",
        "    result_df['mnth'] = Get_month\n",
        "    result_df['day'] = Get_date\n",
        "\n",
        "    # 원-핫 인코딩 처리\n",
        "    result_df = pd.get_dummies(result_df, columns=['day', 'year', 'mnth', 'season'])\n",
        "\n",
        "    # 기존 데이터에 존재하는 모든 컬럼을 포함하도록 처리 (누락된 컬럼은 0으로 채움)\n",
        "    for col in df.columns:\n",
        "        if col not in result_df.columns:\n",
        "            result_df[col] = 0\n",
        "\n",
        "    # 'day_p'와 'snow' 컬럼의 null 값을 0으로 대체\n",
        "    result_df['day_p'].fillna(0, inplace=True)\n",
        "    result_df['snow'].fillna(0, inplace=True)\n",
        "\n",
        "    # 훈련 데이터에 있는 모든 feature를 포함하도록 예측 데이터를 조정\n",
        "    for col in df.drop(['CNT'], axis=1).columns:\n",
        "        if col not in result_df.columns:\n",
        "            result_df[col] = 0  # 없는 feature는 0으로 채웁니다.\n",
        "\n",
        "    # 예측 데이터에 있는 모든 feature가 훈련 데이터에도 있는지 확인하고, 그렇지 않은 경우 해당 feature를 제거\n",
        "    columns_to_remove = [col for col in result_df.columns if col not in df.drop(['CNT'], axis=1).columns]\n",
        "    result_df.drop(columns=columns_to_remove, inplace=True)\n",
        "\n",
        "    return result_df\n",
        "\n",
        "def load_and_process_data():\n",
        "    # 데이터 로딩\n",
        "    df = pd.read_csv('https://raw.githubusercontent.com/rich-hyun/Battle-of-the-Strongest-Statisticians/main/18-23%20Seoul_day.csv')\n",
        "\n",
        "    # 'date' 컬럼을 날짜 형식으로 변환하고 연, 월, 일 정보 추출\n",
        "    df['date'] = pd.to_datetime(df['date'])\n",
        "    df['year'] = df['date'].dt.year\n",
        "    df['mnth'] = df['date'].dt.month\n",
        "    df['day'] = df['date'].dt.day\n",
        "\n",
        "    # 필요한 컬럼 선택\n",
        "    selected_columns = ['day', 'year', 'mnth', 'avg_tmp', 'day_p', 'avg_wind', 'avg_rhum', 't_sd', 'snow', 'avg_gtmp', 'season', 'CNT']\n",
        "    df = df[selected_columns]\n",
        "\n",
        "    # 'CNT' 컬럼의 쉼표 제거하고 실수형으로 변환\n",
        "    df['CNT'] = df['CNT'].str.replace(',', '').astype(float)\n",
        "\n",
        "    # 범주형 데이터 원-핫 인코딩 처리 (필요한 컬럼에 대해서만 실행)\n",
        "    df = pd.get_dummies(df, columns=['day', 'year', 'mnth', 'season'])\n",
        "\n",
        "    # 결측치 처리 (평균값으로 대체)\n",
        "    df = df.fillna(df.mean())\n",
        "\n",
        "    # X와 y 데이터 설정\n",
        "    X = df.drop(['CNT'], axis=1).values.astype(np.float32)\n",
        "    y = df['CNT'].values.astype(np.float32)\n",
        "\n",
        "    return X, y, df\n",
        "\n",
        "# 모델 훈련 및 예측 함수\n",
        "def train_and_predict_model(model, X_train, X_test, y_train, y_test, data, df, model_name):\n",
        "    model.fit(X_train, y_train)\n",
        "    y_pred = model.predict(X_test)\n",
        "    result_df = process_and_encode_data(data, df)\n",
        "    predicted_CNT = model.predict(result_df.values.astype(np.float32))\n",
        "    predicted_CNT_rounded = round(predicted_CNT[0])\n",
        "    print(f\"{model_name} predicted_cnt(rounded): {predicted_CNT_rounded}\")\n",
        "\n",
        "# 모델별 훈련 및 예측 함수 정의\n",
        "def train_and_predict_lgbm(X_train, X_test, y_train, y_test, data, df):\n",
        "    model = lgb.LGBMRegressor(max_depth=30, random_state=42)\n",
        "    train_and_predict_model(model, X_train, X_test, y_train, y_test, data, df, \"LightGBM\")\n",
        "\n",
        "def train_and_predict_xgboost(X_train, X_test, y_train, y_test, data, df):\n",
        "    model = xgb.XGBRegressor(learning_rate=0.2, max_depth=3, n_estimators=100, random_state=42)\n",
        "    train_and_predict_model(model, X_train, X_test, y_train, y_test, data, df, \"XGBoost\")\n",
        "\n",
        "def train_and_predict_rf(X_train, X_test, y_train, y_test, data, df):\n",
        "    model = RandomForestRegressor(n_estimators=300, max_features='sqrt', max_depth=20, random_state=42)\n",
        "    train_and_predict_model(model, X_train, X_test, y_train, y_test, data, df, \"Random Forest\")\n",
        "\n",
        "def train_and_predict_gb(X_train, X_test, y_train, y_test, data, df):\n",
        "    model = GradientBoostingRegressor(max_depth=4, n_estimators=200, random_state=42)\n",
        "    train_and_predict_model(model, X_train, X_test, y_train, y_test, data, df, \"Gradient Boosting\")\n",
        "\n",
        "def train_and_predict_dt(X_train, X_test, y_train, y_test, data, df):\n",
        "    model = DecisionTreeRegressor(max_depth=20, min_samples_split=10, random_state=42)\n",
        "    train_and_predict_model(model, X_train, X_test, y_train, y_test, data, df, \"Decision Tree\")\n",
        "\n",
        "def train_and_predict_lr(X_train, X_test, y_train, y_test, data, df):\n",
        "    model = LinearRegression()\n",
        "    train_and_predict_model(model, X_train, X_test, y_train, y_test, data, df, \"Linear Regression\")\n",
        "\n",
        "# 데이터 로딩 및 처리\n",
        "X, y, df = load_and_process_data()\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# 모델 선택에 따라 해당 모델 훈련 및 예측\n",
        "if Get_selectedModel == 1:\n",
        "    train_and_predict_lgbm(X_train, X_test, y_train, y_test, data, df)\n",
        "\n",
        "elif Get_selectedModel == 2:\n",
        "    train_and_predict_xgboost(X_train, X_test, y_train, y_test, data, df)\n",
        "\n",
        "elif Get_selectedModel == 3:\n",
        "    train_and_predict_rf(X_train, X_test, y_train, y_test, data, df)\n",
        "\n",
        "elif Get_selectedModel == 4:\n",
        "    train_and_predict_gb(X_train, X_test, y_train, y_test, data, df)\n",
        "\n",
        "elif Get_selectedModel == 5:\n",
        "    train_and_predict_dt(X_train, X_test, y_train, y_test, data, df)\n",
        "\n",
        "elif Get_selectedModel == 6:\n",
        "    train_and_predict_lr(X_train, X_test, y_train, y_test, data, df)\n",
        "\n",
        "else:\n",
        "    print(\"error_wrong_number\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4HG3rXypv4Sc",
        "outputId": "9ec0bdb4-e255-42f4-d1ee-9d3bc92056a8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023\n",
            "11\n",
            "14\n",
            "XGBoost predicted_cnt(rounded): 90079\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#종합 파일 연습용"
      ],
      "metadata": {
        "id": "rROMLDyE1PMr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import xgboost as xgb\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "import numpy as np\n",
        "\n",
        "#데이터 값 받기\n",
        "Get_year = int(input())\n",
        "Get_month = int(input())\n",
        "Get_date = int(input())\n",
        "Get_selectedCity=0\n",
        "Get_selectedModel=2\n",
        "\n",
        "data = \"2023-10-27 서울 랜덤포레스트 서울(108) 2023-10-27 15.7 NULL 1.9 71.1 10.07 NULL 16.5\"\n",
        "\n",
        "# 사용자 입력 데이터 처리 함수\n",
        "def process_user_data(data):\n",
        "    data_parts = data.split()\n",
        "    year, mnth, day = map(int, data_parts[0].split('-'))\n",
        "    values = [float(val) if val != \"NULL\" else np.nan for val in data_parts[5:12]]\n",
        "    avg_tmp, day_p, avg_wind, avg_rhum, t_sd, snow, avg_gtmp = values\n",
        "\n",
        "    if mnth in [12, 1, 2]:\n",
        "        season = 3\n",
        "    elif mnth in [3, 4, 5]:\n",
        "        season = 0\n",
        "    elif mnth in [6, 7, 8]:\n",
        "        season = 1\n",
        "    elif mnth in [9, 10, 11]:\n",
        "        season = 2\n",
        "    result=pd.DataFrame([{'year': year, 'mnth': mnth, 'day': day, 'avg_tmp': avg_tmp, 'day_p': day_p,\n",
        "                          'avg_wind': avg_wind, 'avg_rhum': avg_rhum, 't_sd': t_sd, 'snow': snow,\n",
        "                          'avg_gtmp': avg_gtmp, 'season': season}])\n",
        "    result = result.fillna(result.mean())\n",
        "    return result\n",
        "\n",
        "def process_and_encode_data(data, df):\n",
        "    # 사용자 데이터 처리\n",
        "    result_df = process_user_data(data)\n",
        "\n",
        "    # 연도, 월, 일 컬럼 추출\n",
        "    result_df['year'] = Get_year\n",
        "    result_df['mnth'] = Get_month\n",
        "    result_df['day'] = Get_date\n",
        "\n",
        "    # 원-핫 인코딩 처리\n",
        "    result_df = pd.get_dummies(result_df, columns=['day', 'year', 'mnth', 'season'])\n",
        "\n",
        "    # 기존 데이터에 존재하는 모든 컬럼을 포함하도록 처리 (누락된 컬럼은 0으로 채움)\n",
        "    for col in df.columns:\n",
        "        if col not in result_df.columns:\n",
        "            result_df[col] = 0\n",
        "\n",
        "    # 'day_p'와 'snow' 컬럼의 null 값을 0으로 대체\n",
        "    result_df['day_p'].fillna(0, inplace=True)\n",
        "    result_df['snow'].fillna(0, inplace=True)\n",
        "\n",
        "    # 훈련 데이터에 있는 모든 feature를 포함하도록 예측 데이터를 조정\n",
        "    for col in df.drop(['CNT'], axis=1).columns:\n",
        "        if col not in result_df.columns:\n",
        "            result_df[col] = 0  # 없는 feature는 0으로 채웁니다.\n",
        "\n",
        "    # 예측 데이터에 있는 모든 feature가 훈련 데이터에도 있는지 확인하고, 그렇지 않은 경우 해당 feature를 제거\n",
        "    columns_to_remove = [col for col in result_df.columns if col not in df.drop(['CNT'], axis=1).columns]\n",
        "    result_df.drop(columns=columns_to_remove, inplace=True)\n",
        "\n",
        "    return result_df\n",
        "\n",
        "def load_and_process_data():\n",
        "    # 데이터 로딩\n",
        "    df = pd.read_csv('https://raw.githubusercontent.com/rich-hyun/Battle-of-the-Strongest-Statisticians/main/18-23%20Seoul_day.csv')\n",
        "\n",
        "    # 'date' 컬럼을 날짜 형식으로 변환하고 연, 월, 일 정보 추출\n",
        "    df['date'] = pd.to_datetime(df['date'])\n",
        "    df['year'] = df['date'].dt.year\n",
        "    df['mnth'] = df['date'].dt.month\n",
        "    df['day'] = df['date'].dt.day\n",
        "\n",
        "    # 필요한 컬럼 선택\n",
        "    selected_columns = ['day', 'year', 'mnth', 'avg_tmp', 'day_p', 'avg_wind', 'avg_rhum', 't_sd', 'snow', 'avg_gtmp', 'season', 'CNT']\n",
        "    df = df[selected_columns]\n",
        "\n",
        "    # 'CNT' 컬럼의 쉼표 제거하고 실수형으로 변환\n",
        "    df['CNT'] = df['CNT'].str.replace(',', '').astype(float)\n",
        "\n",
        "    # 범주형 데이터 원-핫 인코딩 처리 (필요한 컬럼에 대해서만 실행)\n",
        "    df = pd.get_dummies(df, columns=['day', 'year', 'mnth', 'season'])\n",
        "\n",
        "    # 결측치 처리 (평균값으로 대체)\n",
        "    df = df.fillna(df.mean())\n",
        "\n",
        "    # X와 y 데이터 설정\n",
        "    X = df.drop(['CNT'], axis=1).values.astype(np.float32)\n",
        "    y = df['CNT'].values.astype(np.float32)\n",
        "\n",
        "    return X, y, df\n",
        "\n",
        "# 모델 훈련 및 예측 함수\n",
        "def train_and_predict_model(model, X_train, X_test, y_train, y_test, data, df, model_name):\n",
        "    model.fit(X_train, y_train)\n",
        "    y_pred = model.predict(X_test)\n",
        "    result_df = process_and_encode_data(data, df)\n",
        "    predicted_CNT = model.predict(result_df.values.astype(np.float32))\n",
        "    predicted_CNT_rounded = round(predicted_CNT[0])\n",
        "    print(f\"{model_name} 모델에 의한 예상 CNT (반올림): {predicted_CNT_rounded}\")\n",
        "\n",
        "# 모델별 훈련 및 예측 함수 정의\n",
        "def train_and_predict_lgbm(X_train, X_test, y_train, y_test, data, df):\n",
        "    model = lgb.LGBMRegressor(max_depth=30, random_state=42)\n",
        "    train_and_predict_model(model, X_train, X_test, y_train, y_test, data, df, \"LightGBM\")\n",
        "\n",
        "def train_and_predict_xgboost(X_train, X_test, y_train, y_test, data, df):\n",
        "    model = xgb.XGBRegressor(learning_rate=0.2, max_depth=3, n_estimators=100, random_state=42)\n",
        "    train_and_predict_model(model, X_train, X_test, y_train, y_test, data, df, \"XGBoost\")\n",
        "\n",
        "def train_and_predict_rf(X_train, X_test, y_train, y_test, data, df):\n",
        "    model = RandomForestRegressor(n_estimators=300, max_features='sqrt', max_depth=20, random_state=42)\n",
        "    train_and_predict_model(model, X_train, X_test, y_train, y_test, data, df, \"Random Forest\")\n",
        "\n",
        "def train_and_predict_gb(X_train, X_test, y_train, y_test, data, df):\n",
        "    model = GradientBoostingRegressor(max_depth=4, n_estimators=200, random_state=42)\n",
        "    train_and_predict_model(model, X_train, X_test, y_train, y_test, data, df, \"Gradient Boosting\")\n",
        "\n",
        "def train_and_predict_dt(X_train, X_test, y_train, y_test, data, df):\n",
        "    model = DecisionTreeRegressor(max_depth=20, min_samples_split=10, random_state=42)\n",
        "    train_and_predict_model(model, X_train, X_test, y_train, y_test, data, df, \"Decision Tree\")\n",
        "\n",
        "def train_and_predict_lr(X_train, X_test, y_train, y_test, data, df):\n",
        "    model = LinearRegression()\n",
        "    train_and_predict_model(model, X_train, X_test, y_train, y_test, data, df, \"Linear Regression\")\n",
        "\n",
        "# 데이터 로딩 및 처리\n",
        "X, y, df = load_and_process_data()\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# 모델 선택에 따라 해당 모델 훈련 및 예측\n",
        "if Get_selectedModel == 1:\n",
        "    train_and_predict_lgbm(X_train, X_test, y_train, y_test, data, df)\n",
        "\n",
        "elif Get_selectedModel == 2:\n",
        "    train_and_predict_xgboost(X_train, X_test, y_train, y_test, data, df)\n",
        "\n",
        "elif Get_selectedModel == 3:\n",
        "    train_and_predict_rf(X_train, X_test, y_train, y_test, data, df)\n",
        "\n",
        "elif Get_selectedModel == 4:\n",
        "    train_and_predict_gb(X_train, X_test, y_train, y_test, data, df)\n",
        "\n",
        "elif Get_selectedModel == 5:\n",
        "    train_and_predict_dt(X_train, X_test, y_train, y_test, data, df)\n",
        "\n",
        "elif Get_selectedModel == 6:\n",
        "    train_and_predict_lr(X_train, X_test, y_train, y_test, data, df)\n",
        "\n",
        "else:\n",
        "    print(\"잘못된 모델 선택입니다. 1부터 6까지의 숫자 중 하나를 선택해주세요.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ovbgr_ONNzSF",
        "outputId": "ab001897-74de-41c5-8f2e-2ccc4195c56f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023\n",
            "10\n",
            "26\n",
            "XGBoost 모델에 의한 예상 CNT (반올림): 90079\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.linear_model import LinearRegression\n",
        "import lightgbm as lgb\n",
        "import xgboost as xgb\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "\n",
        "# 데이터 로딩\n",
        "df = pd.read_csv('https://raw.githubusercontent.com/rich-hyun/Battle-of-the-Strongest-Statisticians/main/18-23%20Seoul_day.csv')\n",
        "\n",
        "# 'date' 컬럼을 날짜 형식으로 변환하고 연, 월, 일 정보 추출\n",
        "df['date'] = pd.to_datetime(df['date'])\n",
        "df['year'] = df['date'].dt.year\n",
        "df['mnth'] = df['date'].dt.month\n",
        "df['day'] = df['date'].dt.day\n",
        "\n",
        "# 필요한 컬럼 선택\n",
        "selected_columns = ['day', 'year', 'mnth', 'avg_tmp', 'day_p', 'avg_wind', 'avg_rhum', 't_sd', 'snow', 'avg_gtmp', 'season', 'CNT']\n",
        "df = df[selected_columns]\n",
        "\n",
        "# 'CNT' 컬럼의 쉼표 제거하고 실수형으로 변환\n",
        "df['CNT'] = df['CNT'].str.replace(',', '').astype(float)\n",
        "\n",
        "# 범주형 데이터 원-핫 인코딩 처리 (필요한 컬럼에 대해서만 실행)\n",
        "df = pd.get_dummies(df, columns=['day', 'year', 'mnth', 'season'])\n",
        "\n",
        "# 결측치 처리 (평균값으로 대체)\n",
        "df = df.fillna(df.mean())\n",
        "\n",
        "# X와 y 데이터 설정\n",
        "X = df.drop(['CNT'], axis=1).values.astype(np.float32)\n",
        "y = df['CNT'].values.astype(np.float32)\n",
        "\n",
        "# 데이터를 훈련 세트와 테스트 세트로 분할\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# 모델 생성\n",
        "models = {\n",
        "    '랜덤 포레스트': RandomForestRegressor(max_depth=30, n_estimators=200, random_state=42),\n",
        "    '그래디언트 부스팅': GradientBoostingRegressor(max_depth=4, n_estimators=200, random_state=42),\n",
        "    '의사결정 트리': DecisionTreeRegressor(max_depth=20, min_samples_split=10, random_state=42),\n",
        "    '선형 회귀': LinearRegression(),\n",
        "    'LightGBM': lgb.LGBMRegressor(max_depth=30, random_state=42),\n",
        "    'XGBoost': xgb.XGBRegressor(learning_rate=0.2, max_depth=3, n_estimators=100, random_state=42),\n",
        "}\n",
        "\n",
        "results = []\n",
        "\n",
        "for model_name, model in models.items():\n",
        "    model.fit(X_train, y_train)\n",
        "    predictions = model.predict(X_test)\n",
        "    rmse = np.sqrt(mean_squared_error(y_test, predictions))\n",
        "    mae = mean_absolute_error(y_test, predictions)\n",
        "    r_squared = r2_score(y_test, predictions)\n",
        "    results.append({'모델': model_name, 'RMSE': rmse, 'MAE': mae, 'R-squared': r_squared})\n",
        "\n",
        "# 결과 정렬\n",
        "results_sorted = sorted(results, key=lambda x: x['RMSE'])\n",
        "\n",
        "# 결과 출력\n",
        "for result in results_sorted:\n",
        "    print(f\"{result['모델']} 모델:\")\n",
        "    print(f\"RMSE: {result['RMSE']:.2f}\")\n",
        "    print(f\"MAE: {result['MAE']:.2f}\")\n",
        "    print(f\"R-squared: {result['R-squared']:.2f}\")\n",
        "    print(\"\\n\")"
      ],
      "metadata": {
        "id": "-_2_awB10E75"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#모델 모음"
      ],
      "metadata": {
        "id": "kT7dddEcq0Bk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##모델-xgboost, 결과값 출력"
      ],
      "metadata": {
        "id": "kiyF27xbqwwm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#xgboost\n",
        "import xgboost as xgb\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "import numpy as np\n",
        "\n",
        "# 데이터 로딩\n",
        "df = pd.read_csv('https://raw.githubusercontent.com/rich-hyun/Battle-of-the-Strongest-Statisticians/main/18-23%20Seoul_day.csv')\n",
        "\n",
        "# 'date' 컬럼을 날짜 형식으로 변환하고 연, 월, 일 정보 추출\n",
        "df['date'] = pd.to_datetime(df['date'])\n",
        "df['year'] = df['date'].dt.year\n",
        "df['mnth'] = df['date'].dt.month\n",
        "df['day'] = df['date'].dt.day\n",
        "\n",
        "# 필요한 컬럼 선택\n",
        "selected_columns = ['day', 'year', 'mnth', 'avg_tmp', 'day_p', 'avg_wind', 'avg_rhum', 't_sd', 'snow', 'avg_gtmp', 'season', 'CNT']\n",
        "df = df[selected_columns]\n",
        "\n",
        "# 'CNT' 컬럼의 쉼표 제거하고 실수형으로 변환\n",
        "df['CNT'] = df['CNT'].str.replace(',', '').astype(float)\n",
        "\n",
        "# 범주형 데이터 원-핫 인코딩 처리 (필요한 컬럼에 대해서만 실행)\n",
        "df = pd.get_dummies(df, columns=['day', 'year', 'mnth', 'season'])\n",
        "\n",
        "# 결측치 처리 (평균값으로 대체)\n",
        "df = df.fillna(df.mean())\n",
        "\n",
        "# X와 y 데이터 설정\n",
        "X = df.drop(['CNT'], axis=1).values.astype(np.float32)\n",
        "y = df['CNT'].values.astype(np.float32)\n",
        "\n",
        "# 데이터를 훈련 세트와 테스트 세트로 분할\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# XGBoost 모델 훈련\n",
        "xg_model = xgb.XGBRegressor()  # 이 부분을 수정했습니다.\n",
        "xg_model.fit(X_train, y_train)\n",
        "\n",
        "# 훈련된 XGBoost 모델로 예측 수행\n",
        "xg_pred = xg_model.predict(X_test)\n",
        "\n",
        "# 사용자 입력 데이터 처리 함수\n",
        "def process_user_data(data):\n",
        "    data_parts = data.split()\n",
        "    year, mnth, day = map(int, data_parts[0].split('-'))\n",
        "    values = [float(val) if val != \"NULL\" else np.nan for val in data_parts[5:12]]\n",
        "    avg_tmp, day_p, avg_wind, avg_rhum, t_sd, snow, avg_gtmp = values\n",
        "\n",
        "    if mnth in [12, 1, 2]:\n",
        "        season = 3\n",
        "    elif mnth in [3, 4, 5]:\n",
        "        season = 0\n",
        "    elif mnth in [6, 7, 8]:\n",
        "        season = 1\n",
        "    elif mnth in [9, 10, 11]:\n",
        "        season = 2\n",
        "\n",
        "    return pd.DataFrame([{'year': year, 'mnth': mnth, 'day': day, 'avg_tmp': avg_tmp, 'day_p': day_p,\n",
        "                          'avg_wind': avg_wind, 'avg_rhum': avg_rhum, 't_sd': t_sd, 'snow': snow,\n",
        "                          'avg_gtmp': avg_gtmp, 'season': season}])\n",
        "\n",
        "data = \"2023-10-27 서울 랜덤포레스트 서울(108) 2023-10-27 15.7 NULL 1.9 71.1 10.07 NULL 16.5\"\n",
        "result_df = process_user_data(data)\n",
        "\n",
        "# 원-핫 인코딩 처리\n",
        "result_df = pd.get_dummies(result_df, columns=['day', 'year', 'mnth', 'season'])\n",
        "\n",
        "# 기존 데이터에 존재하는 모든 컬럼을 포함하도록 처리 (누락된 컬럼은 0으로 채움)\n",
        "for col in df.columns:\n",
        "    if col not in result_df.columns:\n",
        "        result_df[col] = 0\n",
        "\n",
        "# 컬럼 순서를 맞춤\n",
        "result_df = result_df[df.drop(['CNT'], axis=1).columns]\n",
        "\n",
        "# 결측치 처리 (평균값으로 대체)\n",
        "result_df = result_df.fillna(df.mean())\n",
        "\n",
        "# 사용자 데이터에 대한 예측 수행\n",
        "user_pred = xg_model.predict(result_df.values.astype(np.float32))\n",
        "\n",
        "# 예측값 반올림 후 출력\n",
        "print(f\"Rounded Predicted Count: {round(user_pred[0])}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kRIQz3dc1r6N",
        "outputId": "8bc60bcb-0c1c-4a81-a59e-0ac0c0325450"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Rounded Predicted Count: 153115\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import xgboost as xgb\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "import numpy as np\n",
        "\n",
        "#데이터 값 받기\n",
        "Get_year = 2022\n",
        "Get_month = 10\n",
        "Get_date = 26\n",
        "Get_selectedCity=0\n",
        "Get_selectedModel=1\n",
        "\n",
        "# 사용자 입력 데이터 처리 함수\n",
        "def process_user_data(data):\n",
        "    data_parts = data.split()\n",
        "    year, mnth, day = map(int, data_parts[0].split('-'))\n",
        "    values = [float(val) if val != \"NULL\" else np.nan for val in data_parts[5:12]]\n",
        "    avg_tmp, day_p, avg_wind, avg_rhum, t_sd, snow, avg_gtmp = values\n",
        "\n",
        "    if mnth in [12, 1, 2]:\n",
        "        season = 3\n",
        "    elif mnth in [3, 4, 5]:\n",
        "        season = 0\n",
        "    elif mnth in [6, 7, 8]:\n",
        "        season = 1\n",
        "    elif mnth in [9, 10, 11]:\n",
        "        season = 2\n",
        "\n",
        "    return pd.DataFrame([{'year': year, 'mnth': mnth, 'day': day, 'avg_tmp': avg_tmp, 'day_p': day_p,\n",
        "                          'avg_wind': avg_wind, 'avg_rhum': avg_rhum, 't_sd': t_sd, 'snow': snow,\n",
        "                          'avg_gtmp': avg_gtmp, 'season': season}])\n",
        "\n",
        "data = \"2023-10-27 서울 랜덤포레스트 서울(108) 2023-10-27 15.7 NULL 1.9 71.1 10.07 NULL 16.5\"\n",
        "result_df = process_user_data(data)\n",
        "\n",
        "result_df['year'] = Get_year\n",
        "result_df['mnth'] = Get_month\n",
        "result_df['day'] = Get_date\n",
        "\n",
        "# 원-핫 인코딩 처리\n",
        "result_df = pd.get_dummies(result_df, columns=['day', 'year', 'mnth', 'season'])\n",
        "\n",
        "# 기존 데이터에 존재하는 모든 컬럼을 포함하도록 처리 (누락된 컬럼은 0으로 채움)\n",
        "for col in df.columns:\n",
        "    if col not in result_df.columns:\n",
        "        result_df[col] = 0\n",
        "\n",
        "# 컬럼 순서를 맞춤\n",
        "result_df = result_df[df.drop(['CNT'], axis=1).columns]\n",
        "\n",
        "# 결측치 처리 (평균값으로 대체)\n",
        "result_df = result_df.fillna(df.mean())\n",
        "\n",
        "\n",
        "# 데이터 로딩\n",
        "df = pd.read_csv('https://raw.githubusercontent.com/rich-hyun/Battle-of-the-Strongest-Statisticians/main/18-23%20Seoul_day.csv')\n",
        "\n",
        "# 'date' 컬럼을 날짜 형식으로 변환하고 연, 월, 일 정보 추출\n",
        "df['date'] = pd.to_datetime(df['date'])\n",
        "df['year'] = df['date'].dt.year\n",
        "df['mnth'] = df['date'].dt.month\n",
        "df['day'] = df['date'].dt.day\n",
        "\n",
        "# 필요한 컬럼 선택\n",
        "selected_columns = ['day', 'year', 'mnth', 'avg_tmp', 'day_p', 'avg_wind', 'avg_rhum', 't_sd', 'snow', 'avg_gtmp', 'season', 'CNT']\n",
        "df = df[selected_columns]\n",
        "\n",
        "# 'CNT' 컬럼의 쉼표 제거하고 실수형으로 변환\n",
        "df['CNT'] = df['CNT'].str.replace(',', '').astype(float)\n",
        "\n",
        "# 범주형 데이터 원-핫 인코딩 처리 (필요한 컬럼에 대해서만 실행)\n",
        "df = pd.get_dummies(df, columns=['day', 'year', 'mnth', 'season'])\n",
        "\n",
        "# 결측치 처리 (평균값으로 대체)\n",
        "df = df.fillna(df.mean())\n",
        "\n",
        "# X와 y 데이터 설정\n",
        "X = df.drop(['CNT'], axis=1).values.astype(np.float32)\n",
        "y = df['CNT'].values.astype(np.float32)\n",
        "\n",
        "# 데이터를 훈련 세트와 테스트 세트로 분할\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# 최적의 하이퍼파라미터로 랜덤 포레스트 모델 생성 및 훈련\n",
        "best_rf_model = RandomForestRegressor(n_estimators=300, max_features='sqrt', max_depth=20, random_state=42)\n",
        "best_rf_model.fit(X_train, y_train)\n",
        "\n",
        "# 테스트 데이터에 대한 예측 수행\n",
        "y_pred = best_rf_model.predict(X_test)\n",
        "\n",
        "# 예측 수행\n",
        "predicted_CNT = best_rf_model.predict(result_df.values.astype(np.float32))\n",
        "\n",
        "# 예측 결과를 반올림\n",
        "predicted_CNT_rounded = round(predicted_CNT[0])\n",
        "\n",
        "# 예측 결과 출력\n",
        "print(f\"Predicted CNT (Rounded): {predicted_CNT_rounded}\")\n",
        "\n",
        "# XGBoost 모델 훈련\n",
        "xg_model = xgb.XGBRegressor()  # 이 부분을 수정했습니다.\n",
        "xg_model.fit(X_train, y_train)\n",
        "\n",
        "# 훈련된 XGBoost 모델로 예측 수행\n",
        "xg_pred = xg_model.predict(X_test)\n",
        "\n",
        "# 사용자 데이터에 대한 예측 수행\n",
        "user_pred = xg_model.predict(result_df.values.astype(np.float32))\n",
        "\n",
        "# 예측값 반올림 후 출력\n",
        "print(f\"Rounded Predicted Count: {round(user_pred[0])}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K8zp9zQC8rmU",
        "outputId": "30c0fe1f-a34c-47fa-c2be-6969f467eb47"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted CNT (Rounded): 138320\n",
            "Rounded Predicted Count: 157140\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##여러 모델 돌려봐서 종류 선택하기"
      ],
      "metadata": {
        "id": "eSdIuGw3rFhM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "import lightgbm as lgb\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "\n",
        "# 데이터 로딩\n",
        "df = pd.read_csv('https://raw.githubusercontent.com/rich-hyun/Battle-of-the-Strongest-Statisticians/main/18-23%20Seoul_day.csv')\n",
        "\n",
        "# 'date' 컬럼을 날짜 형식으로 변환하고 연, 월, 일 정보 추출\n",
        "df['date'] = pd.to_datetime(df['date'])\n",
        "df['year'] = df['date'].dt.year\n",
        "df['mnth'] = df['date'].dt.month\n",
        "df['day'] = df['date'].dt.day\n",
        "\n",
        "# 필요한 컬럼 선택\n",
        "selected_columns = ['day', 'year', 'mnth', 'avg_tmp', 'day_p', 'avg_wind', 'avg_rhum', 't_sd', 'snow', 'avg_gtmp', 'season', 'CNT']\n",
        "df = df[selected_columns]\n",
        "\n",
        "# 'CNT' 컬럼의 쉼표 제거하고 실수형으로 변환\n",
        "df['CNT'] = df['CNT'].str.replace(',', '').astype(float)\n",
        "\n",
        "# 범주형 데이터 원-핫 인코딩 처리 (필요한 컬럼에 대해서만 실행)\n",
        "df = pd.get_dummies(df, columns=['day', 'year', 'mnth', 'season'])\n",
        "\n",
        "# 결측치 처리 (평균값으로 대체)\n",
        "df = df.fillna(df.mean())\n",
        "\n",
        "# X와 y 데이터 설정\n",
        "X = df.drop(['CNT'], axis=1).values.astype(np.float32)\n",
        "y = df['CNT'].values.astype(np.float32)\n",
        "\n",
        "# 데이터를 훈련 세트와 테스트 세트로 분할\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# 모델 생성\n",
        "models = {\n",
        "    '랜덤 포레스트': RandomForestRegressor(random_state=42),\n",
        "    'LightGBM': lgb.LGBMRegressor(random_state=42),\n",
        "    '의사결정 트리': DecisionTreeRegressor(random_state=42),\n",
        "    '선형 회귀': LinearRegression(),\n",
        "    '그래디언트 부스팅': GradientBoostingRegressor(random_state=42),\n",
        "    '서포트 벡터 머신': SVR(),\n",
        "    '나이브 베이즈': GaussianNB(),\n",
        "    'MLP 모델': keras.Sequential([\n",
        "        keras.layers.Dense(64, activation='relu', input_shape=(X_train.shape[1],)),\n",
        "        keras.layers.Dense(32, activation='relu'),\n",
        "        keras.layers.Dense(1)\n",
        "    ]),\n",
        "    'RNN 모델': keras.Sequential([\n",
        "        keras.layers.LSTM(64, input_shape=(X_train.shape[1], 1), activation='relu'),\n",
        "        keras.layers.Dense(1)\n",
        "    ])\n",
        "}\n",
        "\n",
        "# 각 모델의 예측값 구하기 및 평가\n",
        "results = []\n",
        "for model_name, model in models.items():\n",
        "    if '모델' in model_name:  # 모델 이름에 '모델'이 포함된 경우만\n",
        "        model.compile(optimizer='adam', loss='mean_squared_error')\n",
        "        model.fit(X_train, y_train, epochs=50, batch_size=32, verbose=0)\n",
        "    else:\n",
        "        model.fit(X_train, y_train)\n",
        "    predictions = model.predict(X_test)\n",
        "    rmse = np.sqrt(mean_squared_error(y_test, predictions))\n",
        "    mae = mean_absolute_error(y_test, predictions)\n",
        "    r_squared = r2_score(y_test, predictions)\n",
        "    results.append({'모델': model_name, 'RMSE': rmse, 'MAE': mae, 'R-squared': r_squared})\n",
        "\n",
        "# 결과 정렬\n",
        "results_sorted = sorted(results, key=lambda x: x['RMSE'])\n",
        "\n",
        "# 결과 출력\n",
        "for result in results_sorted:\n",
        "    print(f\"{result['모델']} 모델:\")\n",
        "    print(f\"RMSE: {result['RMSE']:.2f}\")\n",
        "    print(f\"MAE: {result['MAE']:.2f}\")\n",
        "    print(f\"R-squared: {result['R-squared']:.2f}\")\n",
        "    print(\"\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r_xMa0s0usJe",
        "outputId": "b4954967-7715-4b41-82bb-aae91bed7341"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-8-a4c640d7d993>:29: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df['CNT'] = df['CNT'].str.replace(',', '').astype(float)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000381 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 1270\n",
            "[LightGBM] [Info] Number of data points in the train set: 1604, number of used features: 60\n",
            "[LightGBM] [Info] Start training from score 73745.144015\n",
            "13/13 [==============================] - 0s 3ms/step\n",
            "13/13 [==============================] - 0s 12ms/step\n",
            "LightGBM 모델:\n",
            "RMSE: 14028.69\n",
            "MAE: 8739.13\n",
            "R-squared: 0.92\n",
            "\n",
            "\n",
            "랜덤 포레스트 모델:\n",
            "RMSE: 14558.71\n",
            "MAE: 9299.78\n",
            "R-squared: 0.91\n",
            "\n",
            "\n",
            "그래디언트 부스팅 모델:\n",
            "RMSE: 14760.52\n",
            "MAE: 9625.47\n",
            "R-squared: 0.91\n",
            "\n",
            "\n",
            "의사결정 트리 모델:\n",
            "RMSE: 17496.67\n",
            "MAE: 11544.72\n",
            "R-squared: 0.88\n",
            "\n",
            "\n",
            "선형 회귀 모델:\n",
            "RMSE: 21190.47\n",
            "MAE: 15568.02\n",
            "R-squared: 0.82\n",
            "\n",
            "\n",
            "MLP 모델 모델:\n",
            "RMSE: 38763.39\n",
            "MAE: 30882.75\n",
            "R-squared: 0.39\n",
            "\n",
            "\n",
            "나이브 베이즈 모델:\n",
            "RMSE: 47178.27\n",
            "MAE: 36149.20\n",
            "R-squared: 0.10\n",
            "\n",
            "\n",
            "서포트 벡터 머신 모델:\n",
            "RMSE: 49850.09\n",
            "MAE: 41107.87\n",
            "R-squared: -0.01\n",
            "\n",
            "\n",
            "RNN 모델 모델:\n",
            "RMSE: 59308.35\n",
            "MAE: 45369.29\n",
            "R-squared: -0.43\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.linear_model import LinearRegression\n",
        "import lightgbm as lgb\n",
        "import xgboost as xgb\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "\n",
        "# 데이터 로딩\n",
        "df = pd.read_csv('https://raw.githubusercontent.com/rich-hyun/Battle-of-the-Strongest-Statisticians/main/18-23%20Seoul_day.csv')\n",
        "\n",
        "# 'date' 컬럼을 날짜 형식으로 변환하고 연, 월, 일 정보 추출\n",
        "df['date'] = pd.to_datetime(df['date'])\n",
        "df['year'] = df['date'].dt.year\n",
        "df['mnth'] = df['date'].dt.month\n",
        "df['day'] = df['date'].dt.day\n",
        "\n",
        "# 필요한 컬럼 선택\n",
        "selected_columns = ['day', 'year', 'mnth', 'avg_tmp', 'day_p', 'avg_wind', 'avg_rhum', 't_sd', 'snow', 'avg_gtmp', 'season', 'CNT']\n",
        "df = df[selected_columns]\n",
        "\n",
        "# 'CNT' 컬럼의 쉼표 제거하고 실수형으로 변환\n",
        "df['CNT'] = df['CNT'].str.replace(',', '').astype(float)\n",
        "\n",
        "# 범주형 데이터 원-핫 인코딩 처리 (필요한 컬럼에 대해서만 실행)\n",
        "df = pd.get_dummies(df, columns=['day', 'year', 'mnth', 'season'])\n",
        "\n",
        "# 결측치 처리 (평균값으로 대체)\n",
        "df = df.fillna(df.mean())\n",
        "\n",
        "# X와 y 데이터 설정\n",
        "X = df.drop(['CNT'], axis=1).values.astype(np.float32)\n",
        "y = df['CNT'].values.astype(np.float32)\n",
        "\n",
        "# 데이터를 훈련 세트와 테스트 세트로 분할\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# 모델 생성\n",
        "models = {\n",
        "    '랜덤 포레스트': RandomForestRegressor(random_state=42),\n",
        "    '그래디언트 부스팅': GradientBoostingRegressor(random_state=42),\n",
        "    '의사결정 트리': DecisionTreeRegressor(random_state=42),\n",
        "    '선형 회귀': LinearRegression(),\n",
        "    'LightGBM': lgb.LGBMRegressor(random_state=42),\n",
        "    'XGBoost': xgb.XGBRegressor(random_state=42),\n",
        "    '서포트 벡터 머신': SVR(),\n",
        "    '나이브 베이즈': GaussianNB(),\n",
        "    'MLP 모델': keras.Sequential([\n",
        "        keras.layers.Dense(128, activation='relu', input_shape=(X_train.shape[1],)),\n",
        "        keras.layers.Dropout(0.2),\n",
        "        keras.layers.Dense(64, activation='relu'),\n",
        "        keras.layers.Dense(1)\n",
        "    ]),\n",
        "    '두 번째 신경망 모델': keras.Sequential([\n",
        "        keras.layers.Dense(128, activation='relu', input_shape=(X_train.shape[1],)),\n",
        "        keras.layers.Dropout(0.2),\n",
        "        keras.layers.Dense(64, activation='relu'),\n",
        "        keras.layers.Dense(1)\n",
        "    ])\n",
        "}\n",
        "\n",
        "# 각 모델의 예측값 구하기 및 평가\n",
        "results = []\n",
        "for model_name, model in models.items():\n",
        "    if '모델' in model_name:  # 모델 이름에 '모델'이 포함된 경우만\n",
        "        model.compile(optimizer='adam', loss='mean_squared_error')\n",
        "        model.fit(X_train, y_train, epochs=50, batch_size=32, verbose=0)\n",
        "    else:\n",
        "        model.fit(X_train, y_train)\n",
        "    predictions = model.predict(X_test)\n",
        "    rmse = np.sqrt(mean_squared_error(y_test, predictions))\n",
        "    mae = mean_absolute_error(y_test, predictions)\n",
        "    r_squared = r2_score(y_test, predictions)\n",
        "    results.append({'모델': model_name, 'RMSE': rmse, 'MAE': mae, 'R-squared': r_squared})\n",
        "\n",
        "# 결과 정렬\n",
        "results_sorted = sorted(results, key=lambda x: x['RMSE'])\n",
        "\n",
        "# 결과 출력\n",
        "for result in results_sorted:\n",
        "    print(f\"{result['모델']} 모델:\")\n",
        "    print(f\"RMSE: {result['RMSE']:.2f}\")\n",
        "    print(f\"MAE: {result['MAE']:.2f}\")\n",
        "    print(f\"R-squared: {result['R-squared']:.2f}\")\n",
        "    print(\"\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NJnMzDwtvzez",
        "outputId": "942a80d7-1743-4041-fad2-9f3e254c6d00"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000345 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 1270\n",
            "[LightGBM] [Info] Number of data points in the train set: 1604, number of used features: 60\n",
            "[LightGBM] [Info] Start training from score 73745.144015\n",
            "13/13 [==============================] - 0s 2ms/step\n",
            "13/13 [==============================] - 0s 2ms/step\n",
            "LightGBM 모델:\n",
            "RMSE: 14028.69\n",
            "MAE: 8739.13\n",
            "R-squared: 0.92\n",
            "\n",
            "\n",
            "XGBoost 모델:\n",
            "RMSE: 14087.92\n",
            "MAE: 9023.32\n",
            "R-squared: 0.92\n",
            "\n",
            "\n",
            "랜덤 포레스트 모델:\n",
            "RMSE: 14558.71\n",
            "MAE: 9299.78\n",
            "R-squared: 0.91\n",
            "\n",
            "\n",
            "그래디언트 부스팅 모델:\n",
            "RMSE: 14760.52\n",
            "MAE: 9625.47\n",
            "R-squared: 0.91\n",
            "\n",
            "\n",
            "의사결정 트리 모델:\n",
            "RMSE: 17496.67\n",
            "MAE: 11544.72\n",
            "R-squared: 0.88\n",
            "\n",
            "\n",
            "선형 회귀 모델:\n",
            "RMSE: 21190.47\n",
            "MAE: 15568.02\n",
            "R-squared: 0.82\n",
            "\n",
            "\n",
            "MLP 모델 모델:\n",
            "RMSE: 34823.39\n",
            "MAE: 27349.01\n",
            "R-squared: 0.51\n",
            "\n",
            "\n",
            "두 번째 신경망 모델 모델:\n",
            "RMSE: 35618.15\n",
            "MAE: 28017.73\n",
            "R-squared: 0.48\n",
            "\n",
            "\n",
            "나이브 베이즈 모델:\n",
            "RMSE: 47178.27\n",
            "MAE: 36149.20\n",
            "R-squared: 0.10\n",
            "\n",
            "\n",
            "서포트 벡터 머신 모델:\n",
            "RMSE: 49850.09\n",
            "MAE: 41107.87\n",
            "R-squared: -0.01\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.linear_model import LinearRegression\n",
        "import lightgbm as lgb\n",
        "import xgboost as xgb\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# 데이터 로딩\n",
        "df = pd.read_csv('https://raw.githubusercontent.com/rich-hyun/Battle-of-the-Strongest-Statisticians/main/18-23%20Seoul_day.csv')\n",
        "\n",
        "# 'date' 컬럼을 날짜 형식으로 변환하고 연, 월, 일 정보 추출\n",
        "df['date'] = pd.to_datetime(df['date'])\n",
        "df['year'] = df['date'].dt.year\n",
        "df['mnth'] = df['date'].dt.month\n",
        "df['day'] = df['date'].dt.day\n",
        "\n",
        "# 필요한 컬럼 선택\n",
        "selected_columns = ['day', 'year', 'mnth', 'avg_tmp', 'day_p', 'avg_wind', 'avg_rhum', 't_sd', 'snow', 'avg_gtmp', 'season', 'CNT']\n",
        "df = df[selected_columns]\n",
        "\n",
        "# 'CNT' 컬럼의 쉼표 제거하고 실수형으로 변환\n",
        "df['CNT'] = df['CNT'].str.replace(',', '').astype(float)\n",
        "\n",
        "# 범주형 데이터 Label Encoding 처리 (season 컬럼에만 적용)\n",
        "label_encoder = LabelEncoder()\n",
        "df['season'] = label_encoder.fit_transform(df['season'])\n",
        "\n",
        "# 결측치 처리 (평균값으로 대체)\n",
        "df = df.fillna(df.mean())\n",
        "\n",
        "# X와 y 데이터 설정\n",
        "X = df.drop(['CNT'], axis=1).values.astype(np.float32)\n",
        "y = df['CNT'].values.astype(np.float32)\n",
        "\n",
        "# 데이터를 훈련 세트와 테스트 세트로 분할\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Keras 모델 생성 함수\n",
        "def create_keras_model():\n",
        "    model = keras.Sequential([\n",
        "        keras.layers.Dense(128, activation='relu', input_shape=(X_train.shape[1],)),\n",
        "        keras.layers.Dropout(0.2),\n",
        "        keras.layers.Dense(64, activation='relu'),\n",
        "        keras.layers.Dense(1)\n",
        "    ])\n",
        "    model.compile(optimizer='adam', loss='mean_squared_error')\n",
        "    return model\n",
        "\n",
        "# 모델 생성\n",
        "models = {\n",
        "    '랜덤 포레스트': RandomForestRegressor(random_state=42),\n",
        "    '그래디언트 부스팅': GradientBoostingRegressor(random_state=42),\n",
        "    '의사결정 트리': DecisionTreeRegressor(random_state=42),\n",
        "    '선형 회귀': LinearRegression(),\n",
        "    'LightGBM': lgb.LGBMRegressor(random_state=42),\n",
        "    'XGBoost': xgb.XGBRegressor(random_state=42),\n",
        "    'MLP 모델': KerasRegressor(build_fn=create_keras_model, epochs=50, batch_size=32, verbose=0)\n",
        "}\n",
        "\n",
        "# 하이퍼파라미터 그리드 정의 (튜닝할 매개변수들)\n",
        "param_grids = {\n",
        "    '랜덤 포레스트': {\n",
        "        'n_estimators': [100, 200, 300],\n",
        "        'max_depth': [10, 20, 30]\n",
        "    },\n",
        "    '그래디언트 부스팅': {\n",
        "        'n_estimators': [100, 200, 300],\n",
        "        'max_depth': [3, 4, 5],\n",
        "        'learning_rate': [0.01, 0.1, 0.2]\n",
        "    },\n",
        "    '의사결정 트리': {\n",
        "        'max_depth': [10, 20, 30],\n",
        "        'min_samples_split': [2, 5, 10]\n",
        "    },\n",
        "    '선형 회귀': {},\n",
        "    'LightGBM': {\n",
        "        'n_estimators': [100, 200, 300],\n",
        "        'max_depth': [10, 20, 30],\n",
        "        'learning_rate': [0.01, 0.1, 0.2]\n",
        "    },\n",
        "    'XGBoost': {\n",
        "        'n_estimators': [100, 200, 300],\n",
        "        'max_depth': [3, 4, 5],\n",
        "        'learning_rate': [0.01, 0.1, 0.2]\n",
        "    },\n",
        "    'MLP 모델': {}\n",
        "}\n",
        "\n",
        "# 각 모델에 대한 하이퍼파라미터 튜닝\n",
        "tuned_models = {}\n",
        "for model_name, model in models.items():\n",
        "    if model_name in param_grids:\n",
        "        param_grid = param_grids[model_name]\n",
        "        grid_search = GridSearchCV(model, param_grid, cv=5, scoring='neg_mean_squared_error', n_jobs=-1)\n",
        "        grid_search.fit(X_train, y_train)\n",
        "        best_model = grid_search.best_estimator_\n",
        "        tuned_models[model_name] = best_model\n",
        "    else:\n",
        "        tuned_models[model_name] = model\n",
        "\n",
        "# 각 모델의 예측값 구하기 및 평가\n",
        "results = []\n",
        "for model_name, model in tuned_models.items():\n",
        "    if '모델' in model_name:  # 모델 이름에 '모델'이 포함된 경우만\n",
        "        model.compile(optimizer='adam', loss='mean_squared_error')\n",
        "        model.fit(X_train, y_train, epochs=50, batch_size=32, verbose=0)\n",
        "    else:\n",
        "        model.fit(X_train, y_train)\n",
        "    predictions = model.predict(X_test)\n",
        "    rmse = np.sqrt(mean_squared_error(y_test, predictions))\n",
        "    mae = mean_absolute_error(y_test, predictions)\n",
        "    r_squared = r2_score(y_test, predictions)\n",
        "    results.append({'모델': model_name, 'RMSE': rmse, 'MAE': mae, 'R-squared': r_squared})\n",
        "\n",
        "# 결과 정렬\n",
        "results_sorted = sorted(results, key=lambda x: x['RMSE'])\n",
        "\n",
        "# 결과 출력\n",
        "for result in results_sorted:\n",
        "    print(f\"{result['모델']} 모델:\")\n",
        "    print(f\"RMSE: {result['RMSE']:.2f}\")\n",
        "    print(f\"MAE: {result['MAE']:.2f}\")\n",
        "    print(f\"R-squared: {result['R-squared']:.2f}\")\n",
        "    print(\"\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 261
        },
        "id": "3vQYYcJFx0X4",
        "outputId": "15665c1c-da5d-4b90-b8e9-ff5079b4916a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-b03cfa8401a8>\u001b[0m in \u001b[0;36m<cell line: 57>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;34m'LightGBM'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlgb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLGBMRegressor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m42\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;34m'XGBoost'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mxgb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mXGBRegressor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m42\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m     \u001b[0;34m'MLP 모델'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mKerasRegressor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuild_fn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_keras_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m }\n\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'KerasRegressor' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##하이퍼파라미터 튜닝"
      ],
      "metadata": {
        "id": "G3lQftpZ0UP0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "# 랜덤 포레스트 모델 정의\n",
        "rf_model = RandomForestRegressor(random_state=42)\n",
        "\n",
        "# 하이퍼파라미터 그리드 정의\n",
        "param_grid_rf = {\n",
        "    'n_estimators': [100, 200, 300],\n",
        "    'max_depth': [10, 20, 30]\n",
        "}\n",
        "\n",
        "# GridSearchCV를 사용하여 하이퍼파라미터 튜닝\n",
        "grid_search_rf = GridSearchCV(rf_model, param_grid_rf, cv=5, scoring='neg_mean_squared_error', n_jobs=-1)\n",
        "grid_search_rf.fit(X_train, y_train)\n",
        "\n",
        "# 최적의 모델과 하이퍼파라미터 출력\n",
        "best_rf_model = grid_search_rf.best_estimator_\n",
        "print(\"최적의 랜덤 포레스트 모델:\", best_rf_model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y7vp1F7c0UgS",
        "outputId": "427287d6-9249-4e95-ef41-d82f9108a458"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "최적의 랜덤 포레스트 모델: RandomForestRegressor(max_depth=30, n_estimators=200, random_state=42)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 그래디언트 부스팅 모델 정의\n",
        "gb_model = GradientBoostingRegressor(random_state=42)\n",
        "\n",
        "# 하이퍼파라미터 그리드 정의\n",
        "param_grid_gb = {\n",
        "    'n_estimators': [100, 200, 300],\n",
        "    'max_depth': [3, 4, 5],\n",
        "    'learning_rate': [0.01, 0.1, 0.2]\n",
        "}\n",
        "\n",
        "# GridSearchCV를 사용하여 하이퍼파라미터 튜닝\n",
        "grid_search_gb = GridSearchCV(gb_model, param_grid_gb, cv=5, scoring='neg_mean_squared_error', n_jobs=-1)\n",
        "grid_search_gb.fit(X_train, y_train)\n",
        "\n",
        "# 최적의 모델과 하이퍼파라미터 출력\n",
        "best_gb_model = grid_search_gb.best_estimator_\n",
        "print(\"최적의 그래디언트 부스팅 모델:\", best_gb_model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jCS0MveM0aYk",
        "outputId": "ba0c1119-78ea-485c-ec36-7d52fb879a34"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "최적의 그래디언트 부스팅 모델: GradientBoostingRegressor(max_depth=4, n_estimators=200, random_state=42)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 의사결정 트리 모델 정의\n",
        "dt_model = DecisionTreeRegressor(random_state=42)\n",
        "\n",
        "# 하이퍼파라미터 그리드 정의\n",
        "param_grid_dt = {\n",
        "    'max_depth': [10, 20, 30],\n",
        "    'min_samples_split': [2, 5, 10]\n",
        "}\n",
        "\n",
        "# GridSearchCV를 사용하여 하이퍼파라미터 튜닝\n",
        "grid_search_dt = GridSearchCV(dt_model, param_grid_dt, cv=5, scoring='neg_mean_squared_error', n_jobs=-1)\n",
        "grid_search_dt.fit(X_train, y_train)\n",
        "\n",
        "# 최적의 모델과 하이퍼파라미터 출력\n",
        "best_dt_model = grid_search_dt.best_estimator_\n",
        "print(\"최적의 의사결정 트리 모델:\", best_dt_model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RsnIs3Ho0d9l",
        "outputId": "9c2c0a08-b61e-4e88-9cc6-40ec0b8051a4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "최적의 의사결정 트리 모델: DecisionTreeRegressor(max_depth=20, min_samples_split=10, random_state=42)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import lightgbm as lgb\n",
        "\n",
        "# LightGBM 모델 정의\n",
        "lgb_model = lgb.LGBMRegressor(random_state=42)\n",
        "\n",
        "# 하이퍼파라미터 그리드 정의\n",
        "param_grid_lgb = {\n",
        "    'n_estimators': [100, 200, 300],\n",
        "    'max_depth': [10, 20, 30],\n",
        "    'learning_rate': [0.01, 0.1, 0.2]\n",
        "}\n",
        "\n",
        "# GridSearchCV를 사용하여 하이퍼파라미터 튜닝\n",
        "grid_search_lgb = GridSearchCV(lgb_model, param_grid_lgb, cv=5, scoring='neg_mean_squared_error', n_jobs=-1)\n",
        "grid_search_lgb.fit(X_train, y_train)\n",
        "\n",
        "# 최적의 모델과 하이퍼파라미터 출력\n",
        "best_lgb_model = grid_search_lgb.best_estimator_\n",
        "print(\"최적의 LightGBM 모델:\", best_lgb_model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CBsdc-7_0ff6",
        "outputId": "c9baf7a9-dc94-423d-9f57-f549b12f0f26"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000078 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 1221\n",
            "[LightGBM] [Info] Number of data points in the train set: 1604, number of used features: 11\n",
            "[LightGBM] [Info] Start training from score 73745.144015\n",
            "최적의 LightGBM 모델: LGBMRegressor(max_depth=30, random_state=42)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import xgboost as xgb\n",
        "\n",
        "# XGBoost 모델 정의\n",
        "xgb_model = xgb.XGBRegressor(random_state=42)\n",
        "\n",
        "# 하이퍼파라미터 그리드 정의\n",
        "param_grid_xgb = {\n",
        "    'n_estimators': [100, 200, 300],\n",
        "    'max_depth': [3, 4, 5],\n",
        "    'learning_rate': [0.01, 0.1, 0.2]\n",
        "}\n",
        "\n",
        "# GridSearchCV를 사용하여 하이퍼파라미터 튜닝\n",
        "grid_search_xgb = GridSearchCV(xgb_model, param_grid_xgb, cv=5, scoring='neg_mean_squared_error', n_jobs=-1)\n",
        "grid_search_xgb.fit(X_train, y_train)\n",
        "\n",
        "# 최적의 모델과 하이퍼파라미터 출력\n",
        "best_xgb_model = grid_search_xgb.best_estimator_\n",
        "print(\"최적의 XGBoost 모델:\", best_xgb_model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zGXprwkm0gTh",
        "outputId": "7ce95357-dfb3-4f9a-de0b-678f1c390737"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "최적의 XGBoost 모델: XGBRegressor(base_score=None, booster=None, callbacks=None,\n",
            "             colsample_bylevel=None, colsample_bynode=None,\n",
            "             colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
            "             enable_categorical=False, eval_metric=None, feature_types=None,\n",
            "             gamma=None, grow_policy=None, importance_type=None,\n",
            "             interaction_constraints=None, learning_rate=0.2, max_bin=None,\n",
            "             max_cat_threshold=None, max_cat_to_onehot=None,\n",
            "             max_delta_step=None, max_depth=3, max_leaves=None,\n",
            "             min_child_weight=None, missing=nan, monotone_constraints=None,\n",
            "             multi_strategy=None, n_estimators=100, n_jobs=None,\n",
            "             num_parallel_tree=None, random_state=42, ...)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import Ridge\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "# 선형 회귀 모델 정의 (Ridge Regression)\n",
        "lr_model = Ridge()\n",
        "\n",
        "# 하이퍼파라미터 그리드 정의\n",
        "param_grid_lr = {\n",
        "    'alpha': [0.1, 1.0, 10.0]  # 여기서는 alpha를 조정해보겠습니다.\n",
        "}\n",
        "\n",
        "# GridSearchCV를 사용하여 하이퍼파라미터 튜닝\n",
        "grid_search_lr = GridSearchCV(lr_model, param_grid_lr, cv=5, scoring='neg_mean_squared_error', n_jobs=-1)\n",
        "grid_search_lr.fit(X_train, y_train)\n",
        "\n",
        "# 최적의 모델과 하이퍼파라미터 출력\n",
        "best_lr_model = grid_search_lr.best_estimator_\n",
        "print(\"최적의 선형 회귀 모델 (Ridge Regression):\", best_lr_model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qv3N8bmf1ssz",
        "outputId": "a470494c-123b-4d6c-b6fd-1f5651127efb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "최적의 선형 회귀 모델 (Ridge Regression): Ridge()\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#전체 모델 코드"
      ],
      "metadata": {
        "id": "TkJr19592sIv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.linear_model import LinearRegression\n",
        "import lightgbm as lgb\n",
        "import xgboost as xgb\n",
        "from tensorflow import keras\n",
        "import tensorflow.keras as keras\n",
        "from tensorflow.keras.wrappers.scikit_learn import KerasRegressor\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "\n",
        "# 데이터 로딩\n",
        "df = pd.read_csv('https://raw.githubusercontent.com/rich-hyun/Battle-of-the-Strongest-Statisticians/main/18-23%20Seoul_day.csv')\n",
        "\n",
        "# 'date' 컬럼을 날짜 형식으로 변환하고 연, 월, 일 정보 추출\n",
        "df['date'] = pd.to_datetime(df['date'])\n",
        "df['year'] = df['date'].dt.year\n",
        "df['mnth'] = df['date'].dt.month\n",
        "df['day'] = df['date'].dt.day\n",
        "\n",
        "# 필요한 컬럼 선택\n",
        "selected_columns = ['day', 'year', 'mnth', 'avg_tmp', 'day_p', 'avg_wind', 'avg_rhum', 't_sd', 'snow', 'avg_gtmp', 'season', 'CNT']\n",
        "df = df[selected_columns]\n",
        "\n",
        "# 'CNT' 컬럼의 쉼표 제거하고 실수형으로 변환\n",
        "df['CNT'] = df['CNT'].str.replace(',', '').astype(float)\n",
        "\n",
        "# 범주형 데이터 원-핫 인코딩 처리 (필요한 컬럼에 대해서만 실행)\n",
        "df = pd.get_dummies(df, columns=['day', 'year', 'mnth', 'season'])\n",
        "\n",
        "# 결측치 처리 (평균값으로 대체)\n",
        "df = df.fillna(df.mean())\n",
        "\n",
        "# X와 y 데이터 설정\n",
        "X = df.drop(['CNT'], axis=1).values.astype(np.float32)\n",
        "y = df['CNT'].values.astype(np.float32)\n",
        "\n",
        "# 데이터를 훈련 세트와 테스트 세트로 분할\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# 각 모델에 대한 하이퍼파라미터 그리드 정의\n",
        "param_grids = {\n",
        "    '랜덤 포레스트': {\n",
        "        'n_estimators': [100, 200, 300],\n",
        "        'max_depth': [10, 20, 30]\n",
        "    },\n",
        "    '그래디언트 부스팅': {\n",
        "        'n_estimators': [100, 200, 300],\n",
        "        'max_depth': [3, 4, 5],\n",
        "        'learning_rate': [0.01, 0.1, 0.2]\n",
        "    },\n",
        "    '의사결정 트리': {\n",
        "        'max_depth': [10, 20, 30],\n",
        "        'min_samples_split': [2, 5, 10]\n",
        "    },\n",
        "    '선형 회귀': {},\n",
        "    'LightGBM': {\n",
        "        'n_estimators': [100, 200, 300],\n",
        "        'max_depth': [10, 20, 30],\n",
        "        'learning_rate': [0.01, 0.1, 0.2]\n",
        "    },\n",
        "    'XGBoost': {\n",
        "        'n_estimators': [100, 200, 300],\n",
        "        'max_depth': [3, 4, 5],\n",
        "        'learning_rate': [0.01, 0.1, 0.2]\n",
        "    },\n",
        "    'MLP 모델': {}\n",
        "}\n",
        "\n",
        "# 모델 이름과 최적 하이퍼파라미터 저장\n",
        "best_models = {}\n",
        "\n",
        "# 모델별로 하이퍼파라미터 튜닝과 평가 수행\n",
        "for model_name, param_grid in param_grids.items():\n",
        "    model = None\n",
        "    if model_name == '랜덤 포레스트':\n",
        "        model = RandomForestRegressor(random_state=42)\n",
        "    elif model_name == '그래디언트 부스팅':\n",
        "        model = GradientBoostingRegressor(random_state=42)\n",
        "    elif model_name == '의사결정 트리':\n",
        "        model = DecisionTreeRegressor(random_state=42)\n",
        "    elif model_name == '선형 회귀':\n",
        "        model = LinearRegression()\n",
        "    elif model_name == 'LightGBM':\n",
        "        model = lgb.LGBMRegressor(random_state=42)\n",
        "    elif model_name == 'XGBoost':\n",
        "        model = xgb.XGBRegressor(random_state=42)\n",
        "    elif model_name == 'MLP 모델':\n",
        "        def create_keras_model():\n",
        "            model = keras.Sequential([\n",
        "                keras.layers.Dense(128, activation='relu', input_shape=(X_train.shape[1],)),\n",
        "                keras.layers.Dropout(0.2),\n",
        "                keras.layers.Dense(64, activation='relu'),\n",
        "                keras.layers.Dense(1)\n",
        "            ])\n",
        "            model.compile(optimizer='adam', loss='mean_squared_error')\n",
        "            return model\n",
        "        model = KerasRegressor(build_fn=create_keras_model, epochs=50, batch_size=32, verbose=0)\n",
        "\n",
        "    grid_search = GridSearchCV(model, param_grid, cv=5, scoring='neg_mean_squared_error', n_jobs=-1)\n",
        "    grid_search.fit(X_train, y_train)\n",
        "    best_models[model_name] = grid_search.best_estimator_\n",
        "\n",
        "# 각 모델의 예측값 구하기 및 평가\n",
        "results = []\n",
        "for model_name, model in best_models.items():\n",
        "    model.fit(X_train, y_train)\n",
        "    predictions = model.predict(X_test)\n",
        "    rmse = np.sqrt(mean_squared_error(y_test, predictions))\n",
        "    mae = mean_absolute_error(y_test, predictions)\n",
        "    r_squared = r2_score(y_test, predictions)\n",
        "    results.append({'모델': model_name, 'RMSE': rmse, 'MAE': mae, 'R-squared': r_squared})\n",
        "\n",
        "# 결과를 RMSE 기준으로 정렬\n",
        "results_sorted = sorted(results, key=lambda x: x['RMSE'])\n",
        "\n",
        "# 결과 출력\n",
        "for result in results_sorted:\n",
        "    print(f\"최적의 {result['모델']} 모델:\")\n",
        "    print(f\"RMSE: {result['RMSE']:.2f}\")\n",
        "    print(f\"MAE: {result['MAE']:.2f}\")\n",
        "    print(f\"R-squared: {result['R-squared']:.2f}\")\n",
        "    print(\"\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 258
        },
        "id": "NozWDEZ-2qBK",
        "outputId": "616785c9-7f97-4b45-c360-2366f3fb3118"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-24-f33137375bd0>\u001b[0m in \u001b[0;36m<cell line: 11>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrappers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscikit_learn\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mKerasRegressor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmean_squared_error\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmean_absolute_error\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mr2_score\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow.keras.wrappers'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.linear_model import LinearRegression\n",
        "import lightgbm as lgb\n",
        "import xgboost as xgb\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "\n",
        "# 데이터 로딩\n",
        "df = pd.read_csv('https://raw.githubusercontent.com/rich-hyun/Battle-of-the-Strongest-Statisticians/main/18-23%20Seoul_day.csv')\n",
        "\n",
        "# 'date' 컬럼을 날짜 형식으로 변환하고 연, 월, 일 정보 추출\n",
        "df['date'] = pd.to_datetime(df['date'])\n",
        "df['year'] = df['date'].dt.year\n",
        "df['mnth'] = df['date'].dt.month\n",
        "df['day'] = df['date'].dt.day\n",
        "\n",
        "# 필요한 컬럼 선택\n",
        "selected_columns = ['day', 'year', 'mnth', 'avg_tmp', 'day_p', 'avg_wind', 'avg_rhum', 't_sd', 'snow', 'avg_gtmp', 'season', 'CNT']\n",
        "df = df[selected_columns]\n",
        "\n",
        "# 'CNT' 컬럼의 쉼표 제거하고 실수형으로 변환\n",
        "df['CNT'] = df['CNT'].str.replace(',', '').astype(float)\n",
        "\n",
        "# 범주형 데이터 원-핫 인코딩 처리 (필요한 컬럼에 대해서만 실행)\n",
        "df = pd.get_dummies(df, columns=['day', 'year', 'mnth', 'season'])\n",
        "\n",
        "# 결측치 처리 (평균값으로 대체)\n",
        "df = df.fillna(df.mean())\n",
        "\n",
        "# X와 y 데이터 설정\n",
        "X = df.drop(['CNT'], axis=1).values.astype(np.float32)\n",
        "y = df['CNT'].values.astype(np.float32)\n",
        "\n",
        "# 데이터를 훈련 세트와 테스트 세트로 분할\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# 모델 생성\n",
        "models = {\n",
        "    '랜덤 포레스트': RandomForestRegressor(max_depth=30, n_estimators=200, random_state=42),\n",
        "    '그래디언트 부스팅': GradientBoostingRegressor(max_depth=4, n_estimators=200, random_state=42),\n",
        "    '의사결정 트리': DecisionTreeRegressor(max_depth=20, min_samples_split=10, random_state=42),\n",
        "    '선형 회귀': LinearRegression(),\n",
        "    'LightGBM': lgb.LGBMRegressor(max_depth=30, random_state=42),\n",
        "    'XGBoost': xgb.XGBRegressor(learning_rate=0.2, max_depth=3, n_estimators=100, random_state=42),\n",
        "}\n",
        "\n",
        "results = []\n",
        "\n",
        "for model_name, model in models.items():\n",
        "    model.fit(X_train, y_train)\n",
        "    predictions = model.predict(X_test)\n",
        "    rmse = np.sqrt(mean_squared_error(y_test, predictions))\n",
        "    mae = mean_absolute_error(y_test, predictions)\n",
        "    r_squared = r2_score(y_test, predictions)\n",
        "    results.append({'모델': model_name, 'RMSE': rmse, 'MAE': mae, 'R-squared': r_squared})\n",
        "\n",
        "# 결과 정렬\n",
        "results_sorted = sorted(results, key=lambda x: x['RMSE'])\n",
        "\n",
        "# 결과 출력\n",
        "for result in results_sorted:\n",
        "    print(f\"{result['모델']} 모델:\")\n",
        "    print(f\"RMSE: {result['RMSE']:.2f}\")\n",
        "    print(f\"MAE: {result['MAE']:.2f}\")\n",
        "    print(f\"R-squared: {result['R-squared']:.2f}\")\n",
        "    print(\"\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TTIDBzy73mAS",
        "outputId": "d64ec62c-0b54-4835-e95e-54cb7b2c0859"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-25-ef11856c07bd>:25: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df['CNT'] = df['CNT'].str.replace(',', '').astype(float)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000391 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 1270\n",
            "[LightGBM] [Info] Number of data points in the train set: 1604, number of used features: 60\n",
            "[LightGBM] [Info] Start training from score 73745.144015\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "그래디언트 부스팅 모델:\n",
            "RMSE: 13606.55\n",
            "MAE: 8823.61\n",
            "R-squared: 0.92\n",
            "\n",
            "\n",
            "LightGBM 모델:\n",
            "RMSE: 14028.69\n",
            "MAE: 8739.13\n",
            "R-squared: 0.92\n",
            "\n",
            "\n",
            "랜덤 포레스트 모델:\n",
            "RMSE: 14460.43\n",
            "MAE: 9301.26\n",
            "R-squared: 0.91\n",
            "\n",
            "\n",
            "XGBoost 모델:\n",
            "RMSE: 14550.40\n",
            "MAE: 9407.87\n",
            "R-squared: 0.91\n",
            "\n",
            "\n",
            "의사결정 트리 모델:\n",
            "RMSE: 16968.68\n",
            "MAE: 11251.30\n",
            "R-squared: 0.88\n",
            "\n",
            "\n",
            "선형 회귀 모델:\n",
            "RMSE: 21190.47\n",
            "MAE: 15568.02\n",
            "R-squared: 0.82\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "mQnRh-p0KXxM"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}